{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Fundamentals for Data Science: Final Exam\n",
    "### Spring 2018\n",
    "\n",
    "\n",
    "## Instructions\n",
    "The final exam is designed to evaluate your grasp of Python theory as well as Python coding.\n",
    "\n",
    "- This is an individual exam.\n",
    "- You have 24 hours to complete the exam, starting from the point at which you first access it.\n",
    "- You will be graded on the quality of your answers.  Use clear, persuasive arguments based on concepts we covered in class.\n",
    "- Please double-click the markdown cells where it says \"Your answer here\" to input answers (if you need more cells please make them markdown cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Praba Santhanakrishnan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: General Questions (21 pts )\n",
    "\n",
    "a) The following method is part of a larger program used by a mobile phone company.  It will work when an object of type MobileDevice or of type ServiceContract is passed in.  This is a demonstration of (select all that apply):\n",
    "\n",
    "    1. Inheritance\n",
    "    2. Polymorphism\n",
    "    3. Duck typing\n",
    "    4. Top-down design\n",
    "    5. Functional programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Method:\n",
    "\n",
    "def add_to_cart(item):\n",
    "    cart.append(item)\n",
    "    total += item.price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a) Duck Typing - Python doesnt check for variable type before running the function. In this case Item variable could be either MobileDevice or of type ServiceContract. It will work fine in both these cases.\n",
    "\n",
    "    Polymorphism - As long as the passed object has the price attribute and can be passed in to the append method of cart , this will work , the ability to have the single interface to different types makes it the demonstration of polymorphism.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Suppose you have a long list of digits (0-9) that you want to write to a file.  Would it be more efficient to use ASCII or UTF-8 as an encoding?  How could you create an even smaller binary file to store the information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- b) It would be efficient to use ASCII encoding for storing digits , though both UTF-8 and ASCII will take same size , it will be faster to access ASCII encoded file compared to UTF-8.\n",
    "\n",
    "Convert the digits into binary using bytearray and store it in the binary format by opening the file as binary file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) You are part of a team working on a spreadsheet program that is written in Python 3.  The program includes several classes to represent different types of objects that fit into a cell of a spreadsheet.  Give a strong argument for why your team should write an abstract base class to represent such objects and give examples of what should go into such an abstract base class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- c) One would create abstract base class , when you want to group the common attributes and methods in to a base class that every object will inherit from . Each object can have the implementation that are specific and unique to them and doesnt apply to other objects.\n",
    "\n",
    "    In this case with spreadsheet cell , though it can fit different objects in them, all the objects will have few common attributes such as Cell comment , Row number , column number etc. These common attributes and methods must be created in the abstract base class instead of each object having the same atrributes repeated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Explain why NumPy is better than lists for \"vectorized\" math operations. Give an example of an operation that is either impossible or painful to implement using traditional Python lists compared to NumPy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- d) NumPy is faster and much efficient in doing vecotrized operations compared to lists. And it is simple and straightforward to use those operations, whereas in lists , those operations involve more than two or even more operations in many cases to perform the same. For example if you want to add two lists  , you have to do looping over all element in the list. In NumPy you simply add those two vairalbes that hold the numpy array. It is just that simple.\n",
    "\n",
    "    Let's say you want to create a standard deviation for the elements of the list, it takes multiple steps , not easier. You would probably create method for mean that takes the input as the list and computes mean by using sum method and divide by the length of the list. Next , you will have to compute sum of square deviation for each element from the mean in another method and then you would have to calculate the standard deviation from this. But in NumPy , you could simply use std() method to do all of it in just one step and it is faster.\n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) We want a list of the numbers that are the square of nonnegative integer less than 10, but whose squares are greater than 10.  The list comprehension below gives an empty list.  Correct it so that we get the desired output: [16, 25, 36, 49, 64, 81]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x**2 for x in range(10) if x**2 > 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Explain why the following code prints what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'function'>\n"
     ]
    }
   ],
   "source": [
    "def f(): pass\n",
    "print(type(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- f) Here the type of the variable f , which is a function in this case which is being printed as the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) Explain why the following code prints something different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "def f(): pass\n",
    "print(type(f()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- g) Here it is printing the type of the output/return value from this method, since it doesnt have any return value  , it is printing NoneType class. Let's say if this function has the return 100 as the statement inside the method , then type(f()) will be <class 'int'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Data Integrity (25 pts)\n",
    "\n",
    "a) Why is it important to sanity-check your data before you begin your analysis? What could happen if you don't?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a) It is very important to understand the data that we wanted to analyze. If you have the data dictionary to understand what each column means and the data types, it would be a great starting point.   We should do sampling of the data to see what type of values the dataset holds and some statistical analysis on the numerical data (shape, min, max , mean, std, etc). It is good to check if they are continous data or any categorical data. It is also important to check if the data is in the right encoded format especially if it is date column to make the analysis easier and correct. There could also be several missing values and extreme outliers in the data , it is good to check if they make sense for the analysis we are planning to do. There could be duplicates in the data set that might skew our analysis.  So it is very critical to do the sanity check before we start our analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Explain, in your own words, why real-world data is often messy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- b) Real world data is always is incomplete and at times inaccurate. People may not fill in all the required data in the right format and might fill-in incorrectly. Many times data entry errors cause the data to be messy. The format at which the data is collected may be wrong as well. The data could be collected from multiple sources and they might go out of sync many times. Not having standard data dictionary might lead into messier data. If it is unstructured data , the chances of data being messy is higher ,  people can enter any value or text which are most of the times messier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) How do you determine which variables in your dataset you should check for issues prior to starting an analysis? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- c)\n",
    "\n",
    "    1. Check for any specified index variable and the uniqueness of it.\n",
    "    \n",
    "    2. if you have date/time column , it is important to check the format of the data given as it can be represented in many differnt format.\n",
    "\n",
    "    3. Check the variables you are planning to have as part of the outcome or make a decision/calculation based on those variable.\n",
    "    \n",
    "    4. Address fields can be mis reperesented as well , at times in 2 or 3 columns , good to check how the data is represented. Zip can be both numbers or alphanumeric.\n",
    "    \n",
    "    5. Check for categorical variables and check for value_counts\n",
    "    \n",
    "    6. Check for continous variables if they can/need to be binned for group/category analysis\n",
    "    \n",
    "    7. Checking for numerical variables for abnormality / outliers \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) How do you know when you have adequately checked these variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- d) \n",
    "\n",
    " 1. When you have validated the data against the dataset documentation.\n",
    " \n",
    " 2. When you have done exploration of the data using commands such as sample() , value_counts() , describe() , max() , min() , isnull() and basic plots such as histogram , scatter and bar and line charts\n",
    " \n",
    " 3. Cross validation within your data between columns and cross validation outside your data (if applicable/available)\n",
    " \n",
    " 4. Checked for extereme value or outliers\n",
    " \n",
    " 5.  Ideally it is never enough to say it is adequate as it is not possible to check every possible thing in the dataset. Basically , create 2 or 3  key hypotheses based on your understanding of the data , and if they pass then you can comfortably move on to the next stage.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Is it possible to fully vet your data for errors before you begin your analysis? If not, what should you be looking out for while you complete your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- e) It is not possible to fully vet the data for errors before beginning the analysis. Need to look out for missing values and make decision on whether to drop the rows or fill it with some formula or previous or next column values.\n",
    "- Look out for outliers that are extreme values (ex: farthest away from mean if numerical data) \n",
    "- During the analysis , look out for transforming data for example into  multiple columns or group them into one column to make better analysis with the data\n",
    "- Cross validation is the key for the analysis to check if the outcomes are as expected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3:  Elections (24 pts)\n",
    "\n",
    "Consider the following data frame in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>delegates</th>\n",
       "      <th>color</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>marco</td>\n",
       "      <td>165</td>\n",
       "      <td>blue</td>\n",
       "      <td>FL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jeb</td>\n",
       "      <td>0</td>\n",
       "      <td>red</td>\n",
       "      <td>FL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chris</td>\n",
       "      <td>0</td>\n",
       "      <td>white</td>\n",
       "      <td>NJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>donald</td>\n",
       "      <td>1543</td>\n",
       "      <td>white</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ted</td>\n",
       "      <td>559</td>\n",
       "      <td>blue</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>john</td>\n",
       "      <td>161</td>\n",
       "      <td>red</td>\n",
       "      <td>OH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name  delegates  color state\n",
       "0   marco        165   blue    FL\n",
       "1     jeb          0    red    FL\n",
       "2   chris          0  white    NJ\n",
       "3  donald       1543  white    NY\n",
       "4     ted        559   blue    TX\n",
       "5    john        161    red    OH"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "# creating a data frame from scratch - list of lists\n",
    "\n",
    "data = [ ['marco', 165, 'blue', 'FL'], \n",
    "         ['jeb', 0, 'red', 'FL'], \n",
    "         ['chris', 0, 'white', 'NJ'], \n",
    "         ['donald', 1543, 'white', 'NY'],\n",
    "         ['ted', 559, 'blue', 'TX'],\n",
    "         ['john', 161, 'red', 'OH']\n",
    "       ]\n",
    "\n",
    "# create a data frame with column names - list of lists\n",
    "\n",
    "col_names = ['name', 'delegates', 'color', 'state']\n",
    "df = pandas.DataFrame(data, columns=col_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Using bracket indexing in Pandas, show how many delegates `ted` got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "559\n"
     ]
    }
   ],
   "source": [
    "# a) \n",
    "\n",
    "print(df['delegates'][4])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Using bracket indexing in Pandas, show how many total delegates were obtained by candidates whose favorite color is blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "724"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b) Your answer here\n",
    "\n",
    "df[df.color=='blue']['delegates'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Using groupby and aggregate in Pandas, show how many total delegates were obtained by candidates grouped by favorite color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       delegates\n",
      "color           \n",
      "blue         724\n",
      "red          161\n",
      "white       1543\n"
     ]
    }
   ],
   "source": [
    "# c) Your answer here\n",
    "\n",
    "print(df.groupby('color').agg('sum'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Clinical disease data (30 pts)\n",
    "\n",
    "Your boss comes to you Monday morning and says “I figured out our next step; we are going to pivot from an online craft store and become a data center for genetic disease information! I found **ClinVar** which is a repository that contains expert curated data, and it is free for the taking. This is a gold mine! Take a week and tell me what gene and mutation combinations are classified as dangerous.”\n",
    "\n",
    "1)  Look at the sample data set (in the Sample ClinVar data below or in the .txt file) and develop a plan of action to use python to extract and summarize just what your boss wants. **Don’t code**. You can use pseudocode and/or and essay format to generate a plan in 500 words or less. \n",
    "\n",
    "2) Tell us the output that you expect from your planned code\n",
    "\n",
    "**Hints:**  \n",
    "\n",
    "* Look at the sample file carefully. What fields do you want to extract? Are they in the same place every time? What strategy will you use to robustly extract and filter your data of interest? How do you plan to handle missing data?\n",
    "\n",
    "* Filter out junk. Just focus on what your boss asked for (1) gene name (2) mutation reference. (3) Filter your data to include only mutations that are dangerous as you define it. \n",
    "\n",
    "* Pandas and NumPy parsers correctly recognize the end of each line in in the ClinVar file.\n",
    "\n",
    "* The unit of observation of this dataset is one row per mutation.\n",
    "\n",
    "* While you shouldn't code your analysis, creating a few lines of code while you think through the problem may be helpful (so that you can sanity check that your plan works). So you can experiment, we have included the data file below as a Tab Separated Value file \"Genomics_Questions.txt\". Please do not submit any such code. For example, if I wanted to check that I accurately understand the \"split\" function in the context of this data, I could type:\n",
    "\n",
    "```python\n",
    "sample = \"abc;def;asd\"\n",
    "test = sample.split(';')\n",
    "```\n",
    "\n",
    "**This is a planning question we want you to lay out a plan in text not code.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VCF file description (Summarized from version 4.1)\n",
    "\n",
    "\n",
    "* The VCF specification:\n",
    "\n",
    "VCF is a text file format which contains meta-information lines, a header\n",
    "line, and then data lines each containing information about a position in the genome. The format also has the ability to contain genotype information on samples for each position.\n",
    "\n",
    "* Fixed fields:\n",
    "\n",
    "There are 8 fixed fields per record. All data lines are **tab-delimited**. In all cases, missing values are specified with a dot (‘.’). \n",
    "\n",
    "1. CHROM - chromosome number\n",
    "2. POS - position DNA nuceleotide count (bases) along the chromosome\n",
    "3. ID - The unique identifier for each mutation\n",
    "4. REF - reference base(s) alleles\n",
    "5. ALT - alternate base(s) alleles\n",
    "6. QUAL - Phred scaled quality score\n",
    "7. FILTER - filter status (if position has passed all filters)\n",
    "8. INFO - a semicolon-separated series of  keys with values in the format: <key>=<data>, and specified as <key>=<data name>[data value definition].\n",
    "\n",
    "\n",
    "### INFO field specifications\n",
    "\n",
    "```\n",
    "GENEINFO = <Gene symbol>\n",
    "CLNSIG =  <Variant Clinical Significance (Severity)>[0 - Uncertain significance, 2 - Benign, 5 - Pathogenic, 255 - other]\n",
    "\n",
    "```\n",
    "\n",
    "### Representative/Sample ClinVar data (vcf file format)\n",
    "\n",
    "```\n",
    "##fileformat=VCFv4.0\t\t\t\t\t\t\t\n",
    "##fileDate=20160705\t\t\t\t\t\t\t\n",
    "##source=ClinVar and dbSNP\t\t\t\t\t\t\t\n",
    "##dbSNP_BUILD_ID=147\t\t\t\t\t\t\t\n",
    "#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\n",
    "1\t949523\trs786201005\tC\tT\t.\t.\tGENEINFO=ISG15;CLNSIG=5\n",
    "1\t949696\trs672601345\tC\tCG\t.\t.\tGENEINFO=ISG15;CLNSIG=5;CLNDBN=Cancer\n",
    "1\t949739\trs672601312\tG\tT\t.\t.\tGENEINFO=ISG15;CLNDBN=Cancer\n",
    "1\t955597\trs115173026\tG\tT\t.\t.\tGENEINFO=AGRN;CLNSIG=2; CLNDBN=Cancer\n",
    "1\t955619\trs201073369\tG\tC\t.\t.\tGENEINFO=AGG;CLNDBN=Heart_dis \n",
    "1\t957640\trs6657048\tC\tT\t.\t.\tGENEINFO=AGG;CLNSIG=3;CLNDBN=Heart_dis \n",
    "1\t976059\trs544749044\tC\tT\t.\t.\tGENEINFO=AGG;CLNSIG=0;CLNDBN=Heart_dis \n",
    "```\n",
    "\n",
    "A second version of this file is provided as a .txt file in case you want to load it into your console to test it out. You can use either file for the data modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Plan:    \n",
    "\n",
    "Step 1: Load the data into the pandas dataframe using '\\t' seperator , use skiprows to ignore the first 4 lines and header=(0) for the columns names.\n",
    "\n",
    "Step 2: The columns we are interested are ID - unique identifier for each mutuation and the INFO columns. Create a seperate dataframe to filter out the rest of the data and keep only the columns of interest.\n",
    "\n",
    "Step 3: Do a quick sanity check for missing data and check the uniqunesess of the mutuation Id.\n",
    "\n",
    "Step 4: Next , using split function on the INFO column , seperate the data in to three new columns for GENEINFO , CLNSIG and \n",
    "CLNDBN , for example we coupld apply lambda function to split (split function on ';') the INFO column and use RegEx ([^=]+$) to find the value (after '=' sign) to parse the columns values for GENEINFO , CLNSIG and CLNDBN\n",
    "\n",
    "Step 5: Make sure these three new columns are added to the dataframe appropriately.\n",
    "\n",
    "Step 6: The data we are interested are the ID (Mutuation) and the GENEINFO for the combination to classify as dangerous.\n",
    "\n",
    "Step 7. The following criteria will idenitfy the combination as dangerous disease.\n",
    "        \n",
    "         if CLNSIG == 5\n",
    "         or CLNDBN == 'Cancer' and CLNSIG != 2\n",
    "         or CLNDBN == 'Heart_dis' \n",
    "         \n",
    "  and add any other criteria by looking at the full data especially for CLNDBN column if there are any new diseses that are dangerous\n",
    "  \n",
    "Step 8: Display the result that tells the combination of Gene and the Mutuation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " This is the output I would expect to see based on the code that would be written for this:\n",
    " \n",
    " \n",
    "     ID            GENEINFO\n",
    "     \n",
    "     rs786201005   ISG15\n",
    "     rs672601345   ISG15\n",
    "     rs672601312   ISG15\n",
    "     rs201073369   AGG\n",
    "     rs6657048     AGG\n",
    "     rs544749044   AGG"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
